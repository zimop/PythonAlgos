{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90086 Workshop 5\n",
    "\n",
    "***\n",
    "In this worksheet, we'll implement a Multilayer perceptron and convolutional neural network (CNN) in Keras—a high-level API for deep learning. We will use MNIST Dataset for experiments.\n",
    "\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- MNIST dataset\n",
    "\n",
    "- Multilayer Perceptron\n",
    "     \n",
    "- Convolutional neural network with standard 2D convolution\n",
    "\n",
    "- Visualize Filters and Feature Maps in CNN\n",
    "     \n",
    "- Bonus: Convolutional neural network with depthwise separable convolution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.13.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST dataset\n",
    "\n",
    "MNIST is a dataset that consists of images of handwritten digits:\n",
    "* the input data are images of handwritten digits (28×28 pixels with a single 8-bit channel)\n",
    "* the target is a label in the set $\\{0, 1, \\ldots, 9\\}$\n",
    "\n",
    "The data is already split into training and test sets. The training set contains 60,000 instances and the test set contains 10,000 instances.\n",
    "\n",
    "<center>Sample images from MNIST test dataset. (MNIST refers to *Modified National Institute of Standards and Technology*.)</center>\n",
    "\n",
    "<img style=\"float: ;\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=500 height=500>\n",
    "\n",
    "(Ref & Image Source: Wikipedia [MNIST database](https://en.wikipedia.org/wiki/MNIST_database))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the data into NumPy arrays using a built-in function from Keras.\n",
    "\n",
    "Keras is an open-source deep learning library written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (60000, 28, 28)\n",
      "test_images shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "print(\"train_images shape:\", train_images.shape)\n",
    "print(\"test_images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data for classification, we need to do some basic pre-processing: \n",
    " * rescale the images so that each pixel is represented as a float between 0 and 1\n",
    " * tranform the input to be a 4D input with number of samples + (rows, cols, channels), so that it can be fed to a layer in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type adjustment & rescale \n",
    "train_images = train_images.astype(float) / 255 #scale the pixel values to be 0-1\n",
    "test_images = test_images.astype(float) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Small detour: One reason for why to normalize data inputs</center>\n",
    "\n",
    "<img style=\"float: ;\" src=\"https://i.stack.imgur.com/1AxJq.png\" width=700 height=700>\n",
    "\n",
    "(Image Source: JEREMY JORDAN, [Normalizing your data](https://www.jeremyjordan.me/batch-normalization/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "tranform the input to be a 4D input with number of samples + (rows, cols, channels)\n",
    "\n",
    "***hint: use [ny.expand_dims](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (60000, 28, 28)\n",
      "test_images shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#transform training data\n",
    "#your code\n",
    "\n",
    "\n",
    "#transform testing data\n",
    "#your code\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_images shape:\", train_images.shape)\n",
    "print(\"test_images shape:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below visualises random examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m sample_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(train_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], size\u001b[38;5;241m=\u001b[39mnum_images, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_ids):\n\u001b[1;32m----> 9\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtrain_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$y = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_labels[s]))\n\u001b[0;32m     11\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 4 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x150 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 10\n",
    "fig, axes = plt.subplots(figsize=(1.5*num_images, 1.5), ncols=num_images)\n",
    "\n",
    "# replace: Whether the sample is with or without replacement. \n",
    "# Default is True, meaning that a value of a can be selected multiple times.\n",
    "sample_ids = np.random.choice(train_images.shape[0], size=num_images, replace=False)\n",
    "\n",
    "for i, s in enumerate(sample_ids):\n",
    "    axes[i].imshow(train_images[s,:,:,0], cmap='binary')\n",
    "    axes[i].set_title(\"$y = {}$\".format(train_labels[s]))\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Small detour: Choosing Colormaps in Matplotlib</center>\n",
    "\n",
    "<img style=\"float: ;\" src=\"https://matplotlib.org/3.4.3/_images/sphx_glr_colormaps_003.png\" width=500 height=500>\n",
    "\n",
    "(Image Source: [Choosing Colormaps in Matplotlib](https://matplotlib.org/devdocs/tutorials/colors/colormaps.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we note that the training set is relatively balanced—there are roughly 6000 examples for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_labels, bins=range(11), align='left')\n",
    "plt.xticks(ticks=range(11))\n",
    "plt.title('Distribution of classes in training data')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Digit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilayer perceptron\n",
    "\n",
    "The handwritten digit recognition task is an example of a _multi-class_ classification problem. \n",
    "There are 10 classes—one for each digit $0, 1,\\ldots, 9$.\n",
    "\n",
    "We need to define 10 output units in the output layer and apply a softmax activation function to generate probability distribution of the classes\n",
    "\n",
    "\n",
    "This MLP model can be expressed in Keras as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model\n",
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((28,28,1)),               # Tell Keras the shape of the input array (a single-channel 28×28 image)\n",
    "        layers.Flatten(),                      # Unravel/flatten the input array\n",
    "        layers.Dense(16, activation='relu'),   # Add a fully-connected layer with 16 units and ReLU activation function as the hidden layer\n",
    "        layers.Dense(10, activation='softmax') # Add a fully-connected layer with a softmax activation function\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Small detour: Softmax & ReLU</center>\n",
    "<img style=\"float: left ;\" src=\"https://pic3.zhimg.com/80/v2-998ddf16795db98b980443db952731c2_1440w.jpg?source=1940ef5c\" width=400 height=400>\n",
    "\n",
    "(Image Source: These images appears in many places, including [here](https://www.programmersought.com/article/58194026613/))\n",
    "\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png\" width=300 height=300>\n",
    "\n",
    "(Image Source: [ReLU](https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to specify a loss function. We use categorial [cross_entropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) for this task. We use [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) (Adaptive Moment Estimation) as the optimisation algorithm to update weights of the network and directs Keras to keep track of accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model\n",
    "mlp.compile(optimizer='adam',\n",
    "           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), #use SparseCategoricalCrossentropy because labels are integers. If the labels are one-hot representation, please use CategoricalCrossentropy loss.\n",
    "           metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to fit the `mlp` model using the training data. \n",
    "By setting `batch_size = 100`, each gradient descent step is computed w.r.t. a random batch of 100 training instances.\n",
    "By setting `epochs = 20`, we loop over the complete training data 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "history_mlp = mlp.fit(train_images, train_labels, epochs=20, batch_size=100, \n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below show that the model fit is unlikely to improve significantly with further training. \n",
    "Both the test loss and accuracy have flattened out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_mlp.history['accuracy'], label='Train')\n",
    "plt.plot(history_mlp.history['val_accuracy'], label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and validating accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_mlp.history['loss'], label='Train')\n",
    "plt.plot(history_mlp.history['val_loss'], label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Loss')\n",
    "plt.title('Training and validating loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, all of these observations were based on validation data. The best way to test it is to use the actual data generated earlier:\n",
    "\n",
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model after training\n",
    "test_results = mlp.evaluate(test_images, test_labels)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar result? That's great :)\n",
    "### Make a prediction\n",
    "\n",
    "***Hint: Try refreshing several times to see different test examples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = np.random.choice(test_images.shape[0])\n",
    "digit = test_images[sample_id]\n",
    "\n",
    "plt.imshow(digit.squeeze(), cmap=\"binary\")\n",
    "plt.title(\"$label = {}$\".format(test_labels[sample_id]))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "digit = np.expand_dims(digit, axis=0)\n",
    "\n",
    "# make a prediction for the test sample\n",
    "predict_digit = mlp.predict(digit)\n",
    "\n",
    "for index, i in enumerate(predict_digit[0]):\n",
    "    print (\"probability for the test sample is\", index, \":\",i)\n",
    "\n",
    "print(\"------\")   \n",
    "# The predicted digit (with the highest probability value)\n",
    "print(\"The predicted digit:\",np.argmax(mlp.predict(digit, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use the Softmax activation function in the output layer, so `predict_digit` returns a 1D tensor with 10 elements, corresponding to the probability values of each category. The predicted digit has the highest probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a useful summary, including output shape and number of parameters  of the model architecture using the `summary` method, as shown below.\n",
    "* Try to  compute the parameters of each layer yourself. Note that if you donot count bias, there could be some difference compare with the summary output number (with bias in a layer, the additional NO. parameters is the NO. filters in the layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional neural network with standard 2D convolution \n",
    "\n",
    "Let's now construct a CNN with standard 2D convolution with the following structures for classification on the MNIST dataset.\n",
    "\n",
    "| Number | Layer type    | Specification                                                                           | Keras function |\n",
    "|--------|---------------|-----------------------------------------------------------------------------------------|----------------|\n",
    "| 1      | Convolutional | 8 5×5 filters with a stride of 1 and a ReLU activation function                         | Conv2D         |\n",
    "| 2      | Pooling       | Max pooling with a 2×2 filter and a stride of 2 (implies pooled regions do not overlap) | MaxPooling2D   |\n",
    "| 3      | Convolutional | 16 5×5 filters with a stride of 1 and a ReLU activation function                        | Conv2D         |\n",
    "| 4      | Pooling       | Same specs as pooling layer #1                                                          | MaxPooling2D   |\n",
    "| 5      | Flatten       | Nil                                                                                     | Flatten        |        |\n",
    "| 6      | Dense         | 10 units (one for each target class) with a softmax activation function.                | Dense          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Complete the code block below to instantiate the model in Keras. \n",
    "\n",
    "Hint: check keras documents for usages of the layers, e.g., how to set the hyperparameters.\n",
    "\n",
    "*2D convolutional layer: [layers.Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "\n",
    "*2D Maxpooling layer: [layers.MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((28, 28, 1)),\n",
    "        \n",
    "        #\n",
    "        #your code\n",
    "        #\n",
    "        \n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and training the model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the `cnn` model takes longer than training the `mlp` model on a CPU. \n",
    "You may like to set the number of epochs to a smaller number (e.g. `epochs=10`) if you don't have much time to spare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_cnn = cnn.fit(train_images, train_labels, epochs=10, batch_size=100, \n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the accuracy and loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['accuracy'], label='Train')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and validating accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['loss'], label='Train')\n",
    "plt.plot(history_cnn.history['val_loss'], label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Loss')\n",
    "plt.title('Training and validating loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a useful summary, including output shape and number of parameters  of the model architecture using the `summary` method, as shown below.\n",
    "* Try to  compute the parameters of each layer yourself. Note that if you donot count bias, there could be some difference compare with the summary output number (with bias in a layer, the additional NO. parameters is the NO. filters in the layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "**Question:** How does  `cnn` model compare with  the earlier `mlp` model in terms of number of parameters and testing accuracy ?\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "In the following CNN network, the input is a RGB image with values in the range 0-255. The convolution operation in the convolutional layers (A,B) are standard 2D convolution (i.e., each kernel has the same number of channels of the input, and the kernel goes through local patches of the input to conduct element-wise multiplication and then sum). Each convolutional layer uses a ReLU activation function. The final layer (D) is a fully connected layer with 10 units.\n",
    " \n",
    "![CNN](https://raw.githubusercontent.com/saraao/COMP90086_image/main/cnn.png)\n",
    "\n",
    "i) What is the total number of parameters in this network? Show your work.\n",
    "\n",
    "ii) Suppose you initialise this network so that all parameters are equal to zero. If you run an image through this network, what will be the output of layer B? Give the dimensions of this output and a description of the values you would expect.\n",
    "\n",
    "iii) Now suppose you initialise this network so that all weights are equal to 1 and all biases equal to 0. If you run an image through this network, what is the output of layer B? Describe what you would expect this layer's output to look like.\n",
    "\n",
    "iv) How could you reduce the number of parameters in this network without reducing the number of filters or their effective kernel size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Filters and Feature Maps in CNN\n",
    "\n",
    "We will discover how to develop simple visualizations for filters and feature maps in a CNN. We'll be using the [VGG16 model](https://keras.io/api/applications/vgg/#vgg16-function) provided by Keras, trained on the ImageNet dataset. For more information on the VGG16, see the [paper](https://arxiv.org/abs/1409.1556).\n",
    "\n",
    "This section draws heavily on [Jason Brownlee](https://machinelearningmastery.com/)‘s work.\n",
    "\n",
    "### Load a VGG16 model loaded with pre-trained ImageNet weights\n",
    "\n",
    "If this is the first time that you have loaded the model, the weights will be downloaded from the internet and may take a moment to download depending on the speed of your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pre-trained model, we can use it as the basis for visualizations.\n",
    "\n",
    "### Visualize Filters\n",
    "\n",
    "#### Summarize the model filters\n",
    "We can access all of the layers of the model via the `model.layers` property.\n",
    "\n",
    "Each layer has a `layer.name` property, where the convolutional layers have a naming convolution like `block#_conv#`, where the `#` is an integer. Therefore, we can check the name of each layer and skip any that don’t contain the string `conv`.\n",
    "\n",
    "Each convolutional layer has two sets of weights. One is the block of filters and the other is the block of bias values. These are accessible via the `layer.get_weights()` function. We can retrieve these weights and then summarize their shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all convolutional layers use 3×3 filters.\n",
    "\n",
    "We can see that for the input image with three channels for red, green and blue, each filter has a depth of three (here we are working with a channel-last format). We could visualize one filter as a plot with three images, one for each channel, or compress all three down to a single colour image, or even just look at the first channel and assume the other channels will look the same. The problem is that we have 63 other filters we might like to visualize.\n",
    "\n",
    "We can retrieve the filters from the first layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight values will likely be small positive and negative values centered around 0.0.\n",
    "\n",
    "We can normalize their values to the range 0-1 to make them easy to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be visualising the first six filters out of 64 filters from 1st layer, and plot each of the three channels of each filter. One row for each filter and one column for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # subplot for 6 filters and 3 channels\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "# plot the filters\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in some cases, the filter is the same across the channels (the first row), and in others, the filters differ (the last row).\n",
    "\n",
    "The dark squares indicate small or inhibitory weights and the light squares represent large or excitatory weights. Using this intuition, we can see that the filters on the first row detect a gradient from light in the top left to dark in the bottom right.\n",
    "\n",
    "\n",
    "### Visualize Feature Maps\n",
    "\n",
    "The activation maps, called feature maps, capture the result of applying the filters to input, such as the input image or another feature map.\n",
    "\n",
    "We need a clearer idea of the shape of the feature maps output by each of the convolutional layers and the layer index number so that we can retrieve the appropriate layer output. The block below will enumerate all layers in the model and print the output size or feature map size for each convolutional layer as well as the layer index in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize feature map shapes\n",
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # summarize output shape\n",
    "    print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information and design a new model that is a subset of the layers in the full VGG16 model. The model would have the same input layer as the original model, but the output would be the output of a given convolutional layer, which we know would be the activation of the layer or the feature map.\n",
    "\n",
    "For example, after loading the VGG model, we can define a new model that outputs a feature map from the first convolutional layer (index 1) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine model to output right after the first hidden layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, we need to load the image with the size expected by the model, in this case, 224×224.\n",
    "\n",
    "### Exercise 4: Try replacing it with the image of your choice\n",
    "Download your favourite photo and place it in your current working directory with the filename `harry.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image with the required shape\n",
    "img = load_img('./harry.png', target_size=(224, 224))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(\"Harry (Photographed by Jiayang Ao)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = np.expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to get the feature map. We can do this easy by calling the `model.predict()` function and passing in the prepared single image.\n",
    "\n",
    "We know the result will be a feature map with 224x224x64. We can plot all 64 two-dimensional images as an 8×8 square of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "for i in range(1, square*square+1):\n",
    "    # specify subplot and turn of axis\n",
    "    ax = plt.subplot(square,square,i)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # plot feature maps in grayscale\n",
    "    plt.imshow(feature_maps[0,:,:,i-1] , cmap='gray')\n",
    "\n",
    "# plot all feature maps   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Convolutional neural network with depthwise separable convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise \n",
    "\n",
    "Construct a CNN with the previous architecture (same output size of feature maps in each layer), except that the standard 2D convolution is replaced  with depthwise separable convolution \n",
    "\n",
    "Complete the code block below to instantiate the model in Keras. \n",
    "\n",
    "***Hint: check keras documents for usages of the layers, e.g., how to set the hyperparameters.***\n",
    "\n",
    "- SeparableConv2D layer: [layers.SeparableConv2D](https://keras.io/api/layers/convolution_layers/separable_convolution2d/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_ds = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((28, 28, 1)),\n",
    "        \n",
    "        #\n",
    "        #your code\n",
    "        #\n",
    "    \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and training as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_ds.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history_cnn_ds = cnn_ds.fit(train_images, train_labels, epochs=5, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a useful summary, including output shape and number of parameters  of the model architecture using the `summary` method, as shown below.\n",
    "* Try to  compute the parameters of each layer yourself. Note that if you donot count bias, there could be some difference compare with the summary output number (with bias in a layer, the additional NO. parameters is the NO. filters in the layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_ds.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Question:** How does  `cnn_ds` model compare with  the earlier `cnn` model in terms of number of parameters and testing accuracy ?\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
