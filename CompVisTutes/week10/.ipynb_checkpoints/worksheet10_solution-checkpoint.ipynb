{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d182026",
   "metadata": {},
   "source": [
    "# COMP90086 Workshop 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a60cff",
   "metadata": {},
   "source": [
    "In this workshop, we will discuss generative models for images.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Autoencoders\n",
    "    - Basic Autoencoder\n",
    "    - Convolutional Autoencoder for Image Denoising\n",
    "    - (Extra bonus) Convolutional Variational Autoencoder \n",
    "- GANs\n",
    "    - Play with GANs in your browser\n",
    "    - DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30985f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.13.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b81d3",
   "metadata": {},
   "source": [
    "# 1. Autoencoders\n",
    "\n",
    "Adapted from the Keras Blog and TensorFlow.org tutorials:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://blog.keras.io/building-autoencoders-in-keras.html\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/240px-Keras_logo.svg.png\" width=32/>\n",
    "    View source on blog.keras.io</a>\n",
    "  </td>   \n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/autoencoder\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View source on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c4fd8",
   "metadata": {},
   "source": [
    "An autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. An autoencoder learns to compress the data while minimizing the reconstruction error. \n",
    "\n",
    "![autoencoder](https://blog.keras.io/img/ae/autoencoder_schema.jpg)\n",
    "<center>Image source: the Keras Blog</center>\n",
    "\n",
    "We will introduce autoencoders with three examples: basic autoencoder, image denoising, and convolutional variational autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0c7a3",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset and Prepare the data\n",
    "We will train the basic autoencoder using the MNIST dataset. \n",
    "\n",
    "MNIST is a dataset that consists of images of handwritten digits:\n",
    "* the input data are images of handwritten digits (28×28 pixels with a single 8-bit channel)\n",
    "* the target is a label in the set $\\{0, 1, \\ldots, 9\\}$\n",
    "\n",
    "The data is already split into training and test sets. The training set contains 60,000 instances and the test set contains 10,000 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28150fcb",
   "metadata": {},
   "source": [
    "Let's prepare our input data. We're using MNIST digits. We load the data into NumPy arrays using a built-in function from Keras, and we're discarding the labels (since we're only interested in encoding/decoding the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d3a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (60000, 784)\n",
      "test_images shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Since we only need images from the dataset to encode and decode,\n",
    "# we won't use the labels.\n",
    "(train_data, _), (test_data, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "train_data = train_data.astype('float32') / 255.\n",
    "test_data = test_data.astype('float32') / 255.\n",
    "\n",
    "x_train = train_data.reshape((len(train_data), np.prod(train_data.shape[1:])))\n",
    "x_test = test_data.reshape((len(test_data), np.prod(test_data.shape[1:])))\n",
    "\n",
    "print(\"train_images shape:\", x_train.shape)\n",
    "print(\"test_images shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada2b4a",
   "metadata": {},
   "source": [
    "## (1) Basic autoencoder\n",
    "\n",
    "Let's start simple, with a single fully-connected neural layer as encoder and as decoder.\n",
    "\n",
    "\n",
    "Define an autoencoder with two Dense layers: an `encoder`, which compresses the images into a 32 dimensional latent vector, and a `decoder`, that reconstructs the original image from the latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209b25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the size of our encoded representations\n",
    "latent_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(latent_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872f148",
   "metadata": {},
   "source": [
    "Let's also create a separate encoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad32b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2a717",
   "metadata": {},
   "source": [
    "As well as the decoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6465293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d605f",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder to reconstruct MNIST digits.\n",
    "\n",
    "First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f4c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ec485",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acde66f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.2731 - val_loss: 0.1870\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1698 - val_loss: 0.1530\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1441 - val_loss: 0.1333\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1207\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1175 - val_loss: 0.1118\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1103 - val_loss: 0.1062\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1055 - val_loss: 0.1021\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1020 - val_loss: 0.0992\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0993 - val_loss: 0.0969\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0973 - val_loss: 0.0954\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0960 - val_loss: 0.0942\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0951 - val_loss: 0.0936\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0946 - val_loss: 0.0931\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0942 - val_loss: 0.0928\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0939 - val_loss: 0.0926\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0937 - val_loss: 0.0924\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0936 - val_loss: 0.0922\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0921\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x24665265520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9d6d8",
   "metadata": {},
   "source": [
    "After 20 epochs, the autoencoder seems to reach a stable train/validation loss value of about 0.09. We can try to visualize the reconstructed inputs and the encoded representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee98ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 792us/step\n",
      "313/313 [==============================] - 0s 868us/step\n"
     ]
    }
   ],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ede2f",
   "metadata": {},
   "source": [
    "Define a `display` function to display ten random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd39cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 10 # How many digits we will display\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    images1 = array1[indices, :]\n",
    "    images2 = array2[indices, :]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1.reshape(28, 28))\n",
    "        plt.title(\"original\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2.reshape(28, 28))\n",
    "        plt.gray()\n",
    "        plt.title(\"reconstructed\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6d39f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the test data and the reconstructed results\n",
    "display(x_test, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda9cbd",
   "metadata": {},
   "source": [
    "Here's what we get. The top row is the original digits, and the bottom row is the reconstructed digits. The autoencoder is able to recreate digits quite accurately, although there is some information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d479c37",
   "metadata": {},
   "source": [
    "### Visualise the latent space\n",
    "\n",
    "The autoencoder has learned a latent representation of the images, which is a compressed representation (in this case, just 32 values) that represent the variation in the MNIST images. We can use [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to visualise the latent space. This is a form of dimensionality-reduction that tries to preserve the distances between points as much as possible -- points that are nearby in the latent space will be nearby in the 2D plot. Different digits are shown in different colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d28fd43",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# using t-SNE to visualise the latent space \u001b[39;00m\n\u001b[0;32m      4\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m----> 5\u001b[0m X_tsne \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_imgs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X_tsne[:,\u001b[38;5;241m0\u001b[39m], X_tsne[:,\u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39m test_labels, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrainbow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:952\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    946\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[t-SNE] Indexed \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m samples in \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124ms...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    947\u001b[0m             n_samples, duration\n\u001b[0;32m    948\u001b[0m         )\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    951\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 952\u001b[0m distances_nn \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m duration \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\neighbors\\_base.py:986\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors_graph\u001b[1;34m(self, X, n_neighbors, mode)\u001b[0m\n\u001b[0;32m    983\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_queries \u001b[38;5;241m*\u001b[39m n_neighbors)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 986\u001b[0m     A_data, A_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m     A_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(A_data)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\neighbors\\_base.py:822\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    815\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    818\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 822\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    834\u001b[0m ):\n\u001b[0;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    836\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    837\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:270\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    259\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    260\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly float64 or float32 datasets pairs are supported at this time, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot: X.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and Y.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:575\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\sklearn\\utils\\fixes.py:72\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreadpoolctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_threadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[43m_ThreadpoolInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m                          \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:373\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_modules_with_enum_process_module_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dl_iterate_phdr()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:485\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m         filepath \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[1;32m--> 485\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_module_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m     kernel_32\u001b[38;5;241m.\u001b[39mCloseHandle(h_process)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[0;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[1;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[1;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[1;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[1;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# using t-SNE to visualise the latent space \n",
    "tsne = TSNE(n_components=2) \n",
    "X_tsne = tsne.fit_transform(encoded_imgs) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c= test_labels, cmap='rainbow')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3d5f2",
   "metadata": {},
   "source": [
    "### Exercise 1: Investigate the latent space representation of MNIST digits\n",
    "\n",
    "Try changing the size of the latent space. How does this affect the reconstruction results and the organisation of digits in the latent space? Why?\n",
    "\n",
    "You may notice the representations of different digits overlap in the latent space, particularly when the latent space is very small. Which digits cluster together in the latent space? Do these clusters make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd0bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d1930b",
   "metadata": {},
   "source": [
    "## (2) Convolutional autoencoder for image denoising\n",
    "\n",
    "An autoencoder can also be trained to remove noise from images. In the following section, you will create a noisy version of the MNIST dataset by applying random noise to each image. You will then train a deep convolutional autoencoder for image denoising, mapping noisy digits images from the MNIST dataset to clean digits images.\n",
    "\n",
    "Below we define a `noise` function to add random noise to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "\n",
    "    return np.clip(noisy_array, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21447001",
   "metadata": {},
   "source": [
    "Let's reuse the dataset we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "x_train = np.reshape(train_data, (len(train_data), 28, 28, 1))\n",
    "x_test = np.reshape(test_data, (len(test_data), 28, 28, 1))\n",
    "\n",
    "# Create a copy of the data with added noise\n",
    "noisy_train_data = noise(x_train)\n",
    "noisy_test_data = noise(x_test)\n",
    "\n",
    "# Display the train data and a version of it with added noise\n",
    "display(x_train, noisy_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdffa2e",
   "metadata": {},
   "source": [
    "Can our autoencoder learn to recover the original digits? Let's find out.\n",
    "\n",
    "**Build the autoencoder.**\n",
    "\n",
    "Compared to the previous convolutional autoencoder, in order to improve the quality of the reconstructed, we'll use a slightly different model with more filters per layer:\n",
    "\n",
    "We are going to use the Functional API to build our convolutional autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, x)\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d0b95",
   "metadata": {},
   "source": [
    "Now we can train our autoencoder using the noisy data as our input and the clean data as our target. We want our autoencoder to learn how to denoise the images.\n",
    "\n",
    "**This may take a while, please refer to your tutor's demonstration of the results during the workshop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(\n",
    "    x=noisy_train_data,\n",
    "    y=x_train,\n",
    "    epochs=2,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(noisy_test_data, x_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f21901",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(noisy_test_data)\n",
    "display(noisy_test_data, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576863",
   "metadata": {},
   "source": [
    "## (3) Extra bonus: Convolutional Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd469b9",
   "metadata": {},
   "source": [
    "Variational autoencoders are a slightly more modern and interesting take on autoencoding.\n",
    "\n",
    "What is a variational autoencoder? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\".\n",
    "\n",
    "Because a VAE is a more complex example, your could refer to the standalone script below:\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/cvae\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8d077",
   "metadata": {},
   "source": [
    "# 2. Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c02c7",
   "metadata": {},
   "source": [
    "## (1) Warm-up [Play with GANs in your browser!](https://poloclub.github.io/ganlab/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142be3c",
   "metadata": {},
   "source": [
    "### GANs, Autoencoders and VAEs\n",
    "Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the “adversarial”) in order to generate new, synthetic instances of data that can pass for real data. They are used widely in image generation, video generation and voice generation.\n",
    "\n",
    "It may be useful to compare generative adversarial networks to other neural networks, such as autoencoders and variational autoencoders.\n",
    "\n",
    "Autoencoders encode input data as vectors. They create a hidden, or compressed, representation of the raw data. They are useful in dimensionality reduction; that is, the vector serving as a hidden representation compresses the raw data into a smaller number of salient dimensions. Autoencoders can be paired with a so-called decoder, which allows you to reconstruct input data based on its hidden representation.\n",
    "\n",
    "Variational autoencoders are generative algorithm that add an additional constraint to encoding the input data, namely that the hidden representations are normalized. Variational autoencoders are capable of both compressing data like an autoencoder and synthesizing data like a GAN. However, while GANs generate data in fine, granular detail, images generated by VAEs tend to be more blurred.\n",
    "\n",
    "(Source of the above introduction: [Pathmind](https://wiki.pathmind.com/generative-adversarial-network-gan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd8cad",
   "metadata": {},
   "source": [
    "## (2) DCGAN\n",
    "\n",
    "We will introduce how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/abs/1511.06434) (DCGAN). We will still use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data.\n",
    "\n",
    "Adapted from TensorFlow.org tutorials and Keras.io guides:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/dcgan\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View source on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "   <td>\n",
    "    <a target=\"_blank\" href=\"https://keras.io/guides/writing_a_training_loop_from_scratch/\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/240px-Keras_logo.svg.png\" width=32/>\n",
    "    View source on keras.io</a>\n",
    "  </td>  \n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130937d",
   "metadata": {},
   "source": [
    "### Load and prepare the dataset\n",
    "\n",
    "Let's reimport the dataset to omit the modifications made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_train = (x_train - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = (x_test - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "print(\"train_images shape:\", x_train.shape)\n",
    "print(\"test_images shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c848533",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93d075",
   "metadata": {},
   "source": [
    "### Create the models\n",
    "\n",
    "A GAN is made of two parts: a \"generator\" model that maps points in the latent space to points in image space, a \"discriminator\" model, a classifier that can tell the difference between real images (from the training dataset) and fake images (the output of the generator network).\n",
    "\n",
    "<img style=\"float: ;\" src=\"https://www.tensorflow.org/images/gan/dcgan.gif\" width=600 height=500>\n",
    "<center>(Image courtesy: tensorflow.org)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb6e74",
   "metadata": {},
   "source": [
    "A GAN training loop looks like this:\n",
    "\n",
    "- (1) Train the generator. \n",
    "    - Sample random points in the latent space. \n",
    "    - Turn the points into fake images via the \"generator\" network. \n",
    "    - Get a batch of real images and combine them with the generated images. \n",
    "    - Train the \"generator\" model to \"fool\" the discriminator and classify the fake images as real.\n",
    "\n",
    "- (2) Train the discriminator. \n",
    "    - Sample a batch of random points in the latent space. \n",
    "    - Turn the points into fake images via the \"generator\" model. \n",
    "    - Get a batch of real images and combine them with the generated images. \n",
    "    - Train the \"discriminator\" model to classify generated vs. real images.\n",
    "\n",
    "Let's implement this training loop. \n",
    "\n",
    "#### The Generator\n",
    "\n",
    "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e84ca",
   "metadata": {},
   "source": [
    "Use the (as yet untrained) generator to create an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabf164",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de712025",
   "metadata": {},
   "source": [
    "#### The Discriminator\n",
    "Then, create the discriminator meant to classify fake vs real digits. The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae520e99",
   "metadata": {},
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e69cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2e7da",
   "metadata": {},
   "source": [
    "#### Define the loss and optimizers\n",
    "Define loss functions and optimizers for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0858ab",
   "metadata": {},
   "source": [
    "Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e1d91",
   "metadata": {},
   "source": [
    "Generator loss\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08575f9f",
   "metadata": {},
   "source": [
    "The discriminator and the generator optimizers are different since you will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c35b8",
   "metadata": {},
   "source": [
    "#### Save checkpoints\n",
    "\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d736dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065c1a7",
   "metadata": {},
   "source": [
    "#### Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec62781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24259e2",
   "metadata": {},
   "source": [
    "#### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 # In practice you need at least 20 epochs to generate nice digits.\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress after each epoch\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797607c",
   "metadata": {},
   "source": [
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for each epoch\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a05ac",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. \n",
    "\n",
    "To train your own model, run:\n",
    "\n",
    "`train(train_dataset, EPOCHS)`\n",
    "\n",
    "To restore the latest checkpoint with your own model, run:\n",
    "\n",
    "`checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))`\n",
    "\n",
    "To save time, we will use the prepared checkpoints directly in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Disable Tensorflow verbose logging output\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR) # or any {DEBUG, INFO, WARNING, ERROR, CRITICAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restore the pre-trained checkpoints\n",
    "# checkpoint.restore(\"./pretrained_checkpoints/ckpt-1\") #about 50 epochs\n",
    "checkpoint.restore(\"./pretrained_checkpoints/ckpt-2\") #about 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = generator(seed, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de569765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336a3e9",
   "metadata": {},
   "source": [
    "### Exercise 2: Check the Discriminator output for real and fake images\n",
    "\n",
    "Run some real images (from a test set) and fake images (N random samples from the Generator) through the Discriminator and check their scores. If the Discriminator is well-trained, it should output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb026ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 10 real images for test set and generate 10 fake images from the Generator\n",
    "real_images = x_test[:10]\n",
    "noise = tf.random.normal([10, noise_dim])\n",
    "fake_images = generator(noise, training=False)\n",
    "\n",
    "display(real_images, fake_images.numpy())\n",
    "\n",
    "# Run these real/fake images rough the Discriminator and check their scores.\n",
    "# your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a6f14",
   "metadata": {},
   "source": [
    "The top row is the real images for the test set, and the bottom row is the fake images generated from the Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377e350",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_output = discriminator(x_test[:10], training=False)\n",
    "fake_output = discriminator(fake_images, training=False)\n",
    "\n",
    "print (\"Scores for real images\",real_output)\n",
    "print (\"Scores for fake images\",fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c15e9",
   "metadata": {},
   "source": [
    "### Exercise 3: Check that generated images are not just copies of the training set\n",
    "\n",
    "Is the Generator simply repeating images from its training set? Or has it learned how to create new images?\n",
    "\n",
    "To check this, generate one random image from the Generator. Compute its similarity (e.g., sum of square pixel difference) to every image in the training set. Return the closest match (or 3 closest matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa111bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388a1e3",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5238b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one random image from the Generator\n",
    "noise = tf.random.normal([1,100])\n",
    "gen = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(gen[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "plt.title(\"Generated Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Compute its similarity to every image in the training set. \n",
    "# We use sum of square pixel difference as the similarity here.\n",
    "similarity = []\n",
    "for i in tqdm(range(len(x_train))):\n",
    "    mse = tf.reduce_mean(tf.square(gen - tf.squeeze(x_train[i])))\n",
    "    heapq.heappush(similarity, (mse,i))\n",
    "\n",
    "# Find the three smallest value, i.e. the three most similar index\n",
    "k = 3\n",
    "results = []\n",
    "for _ in range(k):\n",
    "    results.append(heapq.heappop(similarity))\n",
    "\n",
    "# Display the three closest matches found\n",
    "for i, (_,idx) in enumerate(results):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(np.squeeze(x_train[idx]))\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.title(\"The 3 closest matches in the training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d727e1",
   "metadata": {},
   "source": [
    "### Exercise 4: Do a linear walk through the latent space\n",
    "In a well-trained generative model, the latent space should be **continuous** and **complete**. Continuous means that similar images are nearby in the latent space. Complete means that all points in the latent space correspond to real images.\n",
    "\n",
    "A good way to check this is to do a linear walk through the latent space. Select two random points (A,B) in the Generator's input space. Get N equally-spaced points from A to B (e.g., from `numpy.linspace`) and generate an image for each of those input points.\n",
    "\n",
    "If the GAN is well-trained, the images should show a smooth transition from A to B (indicating that the latent space is continuous) and every step between those points should look like a realistic image (indicating that the latent space is complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cae30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896c057",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two random points in the Generator's input space.\n",
    "noise = tf.random.normal([2, 100])\n",
    "\n",
    "# Get 50 equally-spaced points from A to B\n",
    "walking_latent = np.linspace(noise[0], noise[1], 50)\n",
    "walking_latent_images = generator(walking_latent, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(walking_latent_images.shape[0]):\n",
    "    plt.subplot(5, 10, i+1)\n",
    "    plt.imshow(walking_latent_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5459a",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement [the birthday paradox test](https://arxiv.org/abs/1706.08224)\n",
    "\n",
    "Ideally, a generative model should be able to produce a wide variety of images. However, in practice GANs usually show some amount of \"mode collapse\" -- the Generator can only produce a limited number of images. In severe cases of mode collapse, a Generator will only learn to produce a single image.\n",
    "\n",
    "Less-severe cases of mode collapse are hard to detect, but one way to check for this is the [the birthday paradox test](https://arxiv.org/abs/1706.08224). Choose a value of N and generate N random images. See if you can spot any duplicate images in the set of N. The value of N at which you start seeing duplicates gives you an approximation of the number of images the GAN can produce. If you have a 50% chance of duplicates in a set of N, that suggests that the Generator produces approximately N^2 unique different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a51777",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "\n",
    "noise = tf.random.normal([N, noise_dim])\n",
    "generated_images = generator(noise, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(generated_images.shape[0]):\n",
    "    plt.subplot(5, 10, i+1)\n",
    "    plt.imshow(generated_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e814a9b",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "A Generative Adversarial Network is being trained. The developer observes that initially the Generator is creating images that look like patterned snow.  After some training, it is starting to generate blurry images that start to look like the training data. Later the quality of the generated  images increases but the diversity drops.\n",
    "\n",
    "i) How might the developer measure quality?\n",
    "\n",
    "A: Inception score\n",
    "\n",
    "ii) How might the developer measure diversity?\n",
    "\n",
    "A: Birthday paradox\n",
    "\n",
    "iii) Why might diversity drop during the later part of training?\n",
    "\n",
    "A: Discriminator overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8c970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
