{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T8YmTG4K5d1"
   },
   "source": [
    "# COMP90086 Workshop 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmhzR5g9K5d3"
   },
   "source": [
    "In this workshop, you will have some practice of Hough Transform and Feature matching.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Hough Transform\n",
    "    - How Hough Transform works\n",
    "    - Implement Hough transform to detect lines\n",
    "\n",
    "- Feature matching\n",
    "    - Harris corning detection\n",
    "    - SIFT\n",
    "    - FLANN based Matcher\n",
    "    - Feature matching + homography to find objects\n",
    "    \n",
    "- Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "74PSeIyxL_vp"
   },
   "outputs": [],
   "source": [
    "## For colab user, run\n",
    "\n",
    "#! pip install opencv-contrib-python==4.5.2.52\n",
    "\n",
    "## After the update, You must restart the runtime in order to use newly installed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b6he1NotK5d3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2  \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWQMzW27K5d4"
   },
   "source": [
    "# Hough Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrZeY_goK5d4"
   },
   "source": [
    "##  (1) How Hough Transform works\n",
    "\n",
    "6 mins Introduction Video - \"How Hough Transform works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "dKZsIf77K5d5",
    "outputId": "81381d3e-55eb-40f3-8687-b2919dd814ce"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"397\"\n",
       "            src=\"https://www.youtube.com/embed/4zHbI-fFIlI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x2819b22bd00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How Hough Transform works?\n",
    "# By Thales Sehn Körting (YouTube)\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('4zHbI-fFIlI', width=600, height=397)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Implementing a mapping between a Cartesian coordinate system and its parameter space.</center>\n",
    "<center>y=kx+b --> b=-kx+y</center>\n",
    "<center>Lines with slope k that do not exist cannot be described under the parameter space (The corresponding lines of M and Z in parameter space are parallel)</center>\n",
    "<img style=\"float:  \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/Hough.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Cartesian coordinates mapped to a parameter space under a polar coordinate system</center>\n",
    "<center>for each point (x0,y0) --> rθ=x0⋅cosθ+y0⋅sinθ</center>\n",
    "<center>each pair (rθ,θ) represents each line that passes by (x0,y0).</center>\n",
    "<img style=\"float:  \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/hough_space.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>for lines parallel to the X or Y axis can also be well mapped to the parameter space to ensure that they have intersection points</center>\n",
    "<img style=\"float:  \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/hough_a.jpeg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVLl_DkrK5d5"
   },
   "source": [
    "## (2) Implement Hough transform to detect lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTNE6-P_K5d6"
   },
   "source": [
    "First, we detect the edges of the image by using the Canny edge detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "Rrv6tzWBK5d6",
    "outputId": "0e8c5c7e-f6a7-43be-ce93-987544fcb3db"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m)) \n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal image\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    454\u001b[0m     warn_deprecated(\n\u001b[0;32m    455\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    458\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\matplotlib\\pyplot.py:2650\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   2646\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2647\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2648\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[0;32m   2649\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2650\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   2651\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm, aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   2652\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation, alpha\u001b[38;5;241m=\u001b[39malpha, vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   2653\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax, origin\u001b[38;5;241m=\u001b[39morigin, extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   2654\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   2655\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm, filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   2656\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   2657\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2658\u001b[0m     sci(__ret)\n\u001b[0;32m   2659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[0;32m    454\u001b[0m     warn_deprecated(\n\u001b[0;32m    455\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    458\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\matplotlib\\__init__.py:1414\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1416\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1417\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1418\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5487\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5481\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap, norm, interpolation,\n\u001b[0;32m   5482\u001b[0m                       origin, extent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   5483\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   5484\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5485\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 5487\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5488\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5490\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CV\\lib\\site-packages\\matplotlib\\image.py:706\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be converted to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;66;03m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read in an image from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "gray = cv2.imread(os.path.join(rootpath, \"canny_im.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Canny edge detection with OpenCV\n",
    "edge_img = cv2.Canny(gray,100,150,apertureSize=3,L2gradient=True) #two thresholds in Hysteresis Thresholding and Aperture size of the Sobel\n",
    "\n",
    "# Set the Figure size of plotting\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(gray, cmap='gray')  \n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(edge_img, cmap='gray')  \n",
    "plt.title('Canny edge map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPpHbndyK5d7"
   },
   "source": [
    "Then, we apply Standard Hough Line Transform using the OpenCV functions, cv2.HoughLines(image, rho, theta, threshold) with the following arguments:\n",
    "\n",
    "- image: Output of the edge detector, a single-channel binary source image.\n",
    "\n",
    "- rho: The resolution of the parameter ρ in pixels. We use 1 pixel.\n",
    "\n",
    "- theta: The resolution of the parameter θ in radians. We use 1 degree.\n",
    "\n",
    "- threshold: The minimum number of intersections to \"*detect*\" a line. In other words, only those lines are returned that get enough votes (>threshold).\n",
    "\n",
    "\n",
    "For further understanding, see:\n",
    "- [Hough Transform](http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm).\n",
    "- [cv2.HoughLines( )](https://docs.opencv.org/4.5.2/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:  \" src=\"https://docs.opencv.org/4.5.2/Hough_Lines_Tutorial_Theory_0.jpg\" width=300>\n",
    "(Image source: OpenCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the threshold and see what happens!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjy0sToUK5d7"
   },
   "outputs": [],
   "source": [
    "#Implement Hough transform to detect lines in a Canny edge map \n",
    "\n",
    "# Finds lines in a binary image using the standard Hough transform\n",
    "lines = cv2.HoughLines(edge_img, 1, np.pi/180, thres)  \n",
    "# This function outputs vector of detected lines. \n",
    "# Each line is represented by a vector (ρ,θ). \n",
    "# Whereas ρ/rho is the distance from the coordinate origin (0,0) \n",
    "# and θ/theta is the line rotation angle in radians ( 0∼vertical line,π/2∼horizontal line )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQx134chK5d7"
   },
   "source": [
    "Now, we display the result by drawing detected lines on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "aN5o1AUEK5d8",
    "outputId": "e9c52469-1c46-4aa1-b26d-0e563c239f29"
   },
   "outputs": [],
   "source": [
    "# (Optional) To better distinguish the detected lines from the background, \n",
    "# we will draw the lines in colour. \n",
    "# Therefore, we first convert the grayscale map to RGB format.\n",
    "color_img = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB) \n",
    "\n",
    "# Display by drawing the lines\n",
    "for line in lines:\n",
    "    rho,theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    pt1 = (int(x0 + 1000*(-b)),int(y0 + 1000*(a)))\n",
    "    pt2 = (int(x0 - 1000*(-b)),int(y0 - 1000*(a)))\n",
    "    # Draws a line segment connecting two points, colour=(255,0,0) and thickness=2.\n",
    "    cv2.line(color_img,pt1,pt2,(255,0,0),2)\n",
    "\n",
    "plt.imshow(color_img) \n",
    "plt.title(\"Detected Lines (in red)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCXFz3L_K5d8"
   },
   "source": [
    "# Feature matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corners (E and F) are easier to isolate and locate in the image than other features, which makes them better features to detect. \n",
    "\n",
    "(Image: A. Mordvintsev and A. K. Revision). \n",
    "<img style=\"float: \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/building_corner.jpeg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/corner.jpeg\" width=500>\n",
    "(Image: Moravec 1980)\n",
    "\n",
    "Corners are the important features in the image, and they are generally termed as interest points which are invariant to translation, rotation, and illumination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz0-pQtPK5d8"
   },
   "source": [
    "## (1) Harris corner detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz4N63tDK5d8"
   },
   "source": [
    "We apply Harris Corner Detection using the OpenCV functions, cv2.cornerHarris(image, blockSize, ksize, k) with the following arguments:\n",
    "\n",
    "- img: A single-channel float32 input image.\n",
    "- blockSize: Neighborhood size for corner detection.\n",
    "- ksize: Aperture parameter for the Sobel operator.\n",
    "- k: Harris detector free parameter in the equation. Often between 0.04 and 0.06.\n",
    "\n",
    "For further understanding, see:\n",
    "- [cv2.cornerHarris( )](https://docs.opencv.org/4.5.2/dc/d0d/tutorial_py_features_harris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "ewIdvn6lK5d8",
    "outputId": "1f7ef18f-32ec-471b-ee68-ea5b357c4fb2"
   },
   "outputs": [],
   "source": [
    "# Read in an image from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "img = cv2.imread(os.path.join(rootpath, \"central.png\"))\n",
    "\n",
    "# detector parameters\n",
    "block_size = 2\n",
    "sobel_size = 3\n",
    "k = 0.04\n",
    "\n",
    "# Convert the colour from BGR to RGB for display\n",
    "color_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the colour to grayscale & 32-bit float\n",
    "gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray_img = np.float32(gray_img)\n",
    "\n",
    "# Detect corners with custom detector parameters\n",
    "dst = cv2.cornerHarris(gray_img, block_size, sobel_size, k)\n",
    "\n",
    "# Dilate corner image to enhance corner points, not necessary\n",
    "#kernel = np.ones((3, 3), np.uint8) # The larger the size of the kernel, the greater the dilation\n",
    "#dst = cv2.dilate(dst, kernel)\n",
    "# Try to run the above codes to see what happens on the “Detected corners” map.\n",
    "\n",
    "# Create a copy of the image and draw corners on it\n",
    "corner_image = np.copy(color_img)\n",
    "\n",
    "# Vary the threshold according to the image and the number of corners you want to detect\n",
    "# The corners are drawn on the image if they pass the threshold\n",
    "thresh = 0.1\n",
    "corner_image[dst>thresh*dst.max()]=[0,255,0] # marking the corners in Green\n",
    "# Try to change the threshold to see what happens.\n",
    "\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(color_img)\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(dst, cmap='gray')  \n",
    "plt.title('Detected corners')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(corner_image)\n",
    "plt.title('Display strong corners on the original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABsMIFioK5d9"
   },
   "source": [
    "## (2) SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n84yhTmQK5d9"
   },
   "source": [
    "#### Harris corner detection is not scale invariant, so it is not applicable when the scale of the image changes. To find scale-invariant features, an algorithm called Scale Invariant Feature Transform (SIFT) was proposed by D.Lowe.\n",
    "\n",
    "<img style=\"float: \" src=\"https://miro.medium.com/max/1400/1*nkJt5BX6WJEDbVAl-fOE0A.png\" width=500>\n",
    "\n",
    "(Image Source: These images appears in many places, including [here](https://medium.com/jun94-devpblog/cv-12-scale-invariant-local-feature-extraction-2-harris-laplace-170d48ee1bf1))\n",
    "\n",
    "Harris corner point detection is rotationally invariant, but not scale invariant. As shown in the figure below, corner points at small scales may be assumed to be edges when they are zoomed in.\n",
    "\n",
    "\n",
    "For further understanding, see:\n",
    "- [SIFT](https://docs.opencv.org/4.5.2/da/df5/tutorial_py_sift_intro.html)\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [cv2.pyrDown( )](https://docs.opencv.org/4.5.2/d4/d86/group__imgproc__filter.html#gaf9bba239dfca11654cb7f50f889fc2ff)\n",
    "- [cv2.getRotationMatrix2D( )](https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326)\n",
    "- [cv2.warpAffine( )](https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983)\n",
    "- [cv2.drawKeypoints( )](https://docs.opencv.org/4.5.2/d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "B91u7AQiK5d9",
    "outputId": "152f8a86-8c06-4ed1-819f-ec4357cc3306"
   },
   "outputs": [],
   "source": [
    "# Check your cv2 version\n",
    "cv2.__version__  \n",
    "# Note that these were previously only available in the opencv contrib repo, \n",
    "# but the patent expired in the year 2020. So they are now included in the main repo. \n",
    "\n",
    "# For those who are using OpenCV versions released before 2020 (e.g. cv2.__version__ == 4.2.x ), \n",
    "# they might got \"AttributeError: module 'cv2' has no attribute 'SIFT_create' \".  \n",
    "\n",
    "# To solve this problem, one possible solution is to uninstall Anaconda and reinstall to the latest version.\n",
    "# Then follow the Week 1 guide “Getting set up for workshops”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxRG_TPkK5d9"
   },
   "source": [
    "Display original image and scene image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "6jnwrSiUK5d9",
    "outputId": "fd2242c4-2149-48cb-9617-cc75d817070e"
   },
   "outputs": [],
   "source": [
    "# Read in images from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "gray = cv2.imread(os.path.join(rootpath, 'box.png'),cv2.IMREAD_GRAYSCALE)\n",
    "scene_gray = cv2.imread(os.path.join(rootpath, 'box_in_scene.png'),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Optional: Create a modified image by adding scale invariance and rotation invariance\n",
    "\n",
    "#scene_gray = cv2.pyrDown(gray) #blurs an image and downsamples it\n",
    "#rows, cols = scene_gray.shape[:2] #in case this is not a greyscale image\n",
    "#rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1) #calculate an affine matrix of 2D rotation\n",
    "#scene_gray = cv2.warpAffine(scene_gray, rotation_matrix, (cols, rows)) #apply an affine transformation to image\n",
    "\n",
    "\n",
    "# Display original image and scene image\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(gray, cmap='gray')  \n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(scene_gray, cmap='gray')  \n",
    "plt.title('Scene Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "Q1rjjbcQK5d-",
    "outputId": "f660b485-25e9-453c-862e-a68e1ec38f6e"
   },
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create() # if cv2 version >= 4.4.0 \n",
    "# sift = cv2.xfeatures2d.SIFT_create() # if cv2 version = 4.3.x \n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(gray,None)\n",
    "kp2, des2 = sift.detectAndCompute(scene_gray,None)\n",
    "\n",
    "# Draws the small circles on the locations of keypoints without size\n",
    "kp1_without_size = cv2.drawKeypoints(gray,kp1,None\n",
    "                                     #, color = (0, 0, 255) #If you want a specific colour\n",
    "                                    )\n",
    "kp2_without_size = cv2.drawKeypoints(scene_gray,kp2,None\n",
    "                                     #, color = (0, 0, 255) #If you want a specific colour\n",
    "                                    )\n",
    "\n",
    "# Draws a circle with the size of each keypoint and show its orientation\n",
    "kp1_with_size = cv2.drawKeypoints(gray,kp1,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "kp2_with_size = cv2.drawKeypoints(scene_gray,kp2,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "\n",
    "# Display images with&without the size of keypoints \n",
    "plt.subplots(figsize=(15, 10)) \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(kp1_without_size, cmap='gray')  \n",
    "plt.title('Original Image keypoints without size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(kp2_without_size, cmap='gray')  \n",
    "plt.title('Scene Image keypoints without size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(kp1_with_size, cmap='gray')  \n",
    "plt.title('Original Image keypoints with size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(kp2_with_size, cmap='gray')  \n",
    "plt.title('Scene Image keypoints with size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"Number of keypoints detected in the original image: \", len(kp1))\n",
    "print(\"Number of keypoints detected in the Scene image: \", len(kp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4xvROkiK5d-"
   },
   "source": [
    "#### Now we've got the keypoints, descriptors and so on. Now we are going to see how to match keypoints in different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiULcoe-K5d-"
   },
   "source": [
    "## (3) FLANN based Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8U6b6MjK5d-"
   },
   "source": [
    "#### FLANN (Fast Library for Approximate Nearest Neighbors) contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. \n",
    "\n",
    "When performing matching, the KNN algorithm is generally used to find the two nearest neighbours. If the ratio between the closest and second closest is greater than a given value, then we keep this closest value and consider it and its matching point as a good match.\n",
    "\n",
    "For further understanding, see:\n",
    "\n",
    "- [FLANN based Matcher](https://docs.opencv.org/4.5.2/dc/dc3/tutorial_py_matcher.html#flann-based-matcher)\n",
    "\n",
    "More information on the functions used below, see:\n",
    "\n",
    "- [FlannBasedMatcher Class Reference](https://docs.opencv.org/4.5.2/dc/de2/classcv_1_1FlannBasedMatcher.html)\n",
    "- [cv2.drawMatchesKnn( )](https://docs.opencv.org/4.5.2/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "V1J5YlG0K5d-",
    "outputId": "56593495-ccf7-4473-f31a-4ec1c203838d"
   },
   "outputs": [],
   "source": [
    "# FLANN parameters and initialize\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Matching descriptor using KNN algorithm\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Create a mask to draw all good matches\n",
    "matchesMask = []\n",
    "\n",
    "# Store all good matches as per Lowe's Ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "        matchesMask.append([1,0]) # Match\n",
    "    else:\n",
    "        matchesMask.append([0,0]) # Mismatch\n",
    "       \n",
    "        \n",
    "# Draw all good matches\n",
    "draw_params = dict(#matchColor = (0,255,0),  #If you want a specific colour\n",
    "                   #singlePointColor = (255,0,0), #If you want a specific colour\n",
    "                    matchesMask = matchesMask,\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "\n",
    "good_matches = cv2.drawMatchesKnn(gray,kp1,scene_gray,kp2,matches,None,**draw_params)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.imshow(good_matches)\n",
    "plt.title('All good matches')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print total number of good matches between two images\n",
    "print(\"\\nNumber of good matches between two images: \", len(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dMsqzECK5d-"
   },
   "source": [
    "## (4) Feature matching + homography to find objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uskExuKsK5d-"
   },
   "source": [
    "We have found a number of matching points in two images by following the steps above. How do we align one of the images with the other by rotating, transforming, etc.? This is where the homography matrix comes into play.\n",
    "\n",
    "The homography matrix has eight parameters. Since each corresponding pixel point can generate 2 equations (one x, one y), then only a minimum of four pixel points are enough to solve the Homography matrix.\n",
    "\n",
    "\n",
    "More information on the functions used below, see:\n",
    "\n",
    "- [cv2.findHomography( )](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780)\n",
    "\n",
    "- [cv2.warpPerspective( )](https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87)\n",
    "\n",
    "- [cv2.drawMatches( )](https://docs.opencv.org/4.5.2/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "id": "lTUH67bcK5d_",
    "outputId": "32af15c8-1baa-4ac0-cd83-f753840b7b11"
   },
   "outputs": [],
   "source": [
    "# Now we set a condition that at least N matches (defined by MIN_MATCH_NUM) are required to find the object. \n",
    "MIN_MATCH_NUM = 4\n",
    "\n",
    "if len(good)>= MIN_MATCH_NUM:\n",
    "    # If enough matches are found, we extract the positions of the matched keypoints in both images. \n",
    "    # They are passed to find the perspective transformation. \n",
    "    \n",
    "    # Estimate homography between two images\n",
    "    ptsA = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    ptsB = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    H, status = cv2.findHomography(ptsA, \n",
    "                                   ptsB, \n",
    "                                   cv2.RANSAC, \n",
    "                                   ransacReprojThreshold = 5, \n",
    "                                   maxIters = 10) # try to change maxIters and see the effect\n",
    "    # Where H is the resulting single-strain matrix.\n",
    "    # status returns a list of feature points that represent successful matches.\n",
    "    # ptsA, ptsB are keypoints.\n",
    "    # The three parameters cv2.RANSAC, ransacReprojThreshold, maxIters are related to RANSAC.\n",
    "    # ransacReprojThreshold: Maximum reprojection error in the RANSAC algorithm to consider a point as an inlier. \n",
    "    # maxIters: The maximum number of RANSAC-based robust method iterations.\n",
    "    \n",
    "    success = status.ravel().tolist()\n",
    "    \n",
    "    # Draw detected template in scene image\n",
    "    imgOut = cv2.warpPerspective(scene_gray, H, (gray.shape[1],gray.shape[0]),\n",
    "                             flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "    \n",
    "    # Print total number of successful matches between two images\n",
    "    print(\"\\nNumber of successful matches between two images: \", success.count(1)) # Returns the number of 1 in the success list\n",
    "\n",
    "else:\n",
    "    # Otherwise, print that “Not enough matches are found”.\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_NUM) )\n",
    "    success = None\n",
    "\n",
    "\n",
    "# Draw our inliers (if successfully found the object) or all matching keypoints (if failed)\n",
    "draw_params = dict(#matchColor = (0,255,0), # draw in a specific colour\n",
    "                   #singlePointColor = (255,0,0), # draw in a specific colour\n",
    "                   matchesMask = success, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "success_matches = cv2.drawMatches(gray,kp1,scene_gray,kp2,good,None,**draw_params)\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "if success == None:\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All matching keypoints')\n",
    "    plt.axis('off')\n",
    "    \n",
    "else:\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All successful matches')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(imgOut, 'gray')\n",
    "    plt.title('Display detected template in scene image')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKy6M2uDK5d_"
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d25GpcDhK5d_"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "- Implement Canny edge detection and Hough transform to detect the grid in the checkerboard.\n",
    "- Change the threshold of the HoughLines function. What happens as you increase/decrease the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIyhISyzK5d_"
   },
   "outputs": [],
   "source": [
    "# Read in an image from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "checkerboard = cv2.imread(os.path.join(rootpath, \"checkerboard.png\"),cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoAVFGx9K5d_"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjR8TrS3K5d_"
   },
   "source": [
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGW_dJV1K5d_"
   },
   "source": [
    "Answer: fewer/more lines (but with lower thresholds, the lines may be worse, since they will be accepted with fewer \"votes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4zjwt4BK5d_"
   },
   "source": [
    "The code here is only a reference solution. The aim of the exercise is to understand the effect of different parameters on the results. Different parameters are accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "RReTwkkVK5d_",
    "outputId": "56bf6856-3cbc-4004-88a9-9de1e8ced2c3"
   },
   "outputs": [],
   "source": [
    "# Canny edge detection with OpenCV\n",
    "board_edge = cv2.Canny(checkerboard,50,250,apertureSize=3,L2gradient=True)\n",
    "\n",
    "# Sandard Hough transform\n",
    "lines = cv2.HoughLines(board_edge, 1, np.pi/180, 120)  \n",
    "\n",
    "# Convert the grayscale map to RGB format.\n",
    "color_board = cv2.cvtColor(checkerboard, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Drawing the lines\n",
    "for line in lines:\n",
    "    rho,theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    pt1 = (int(x0 + 1000*(-b)),int(y0 + 1000*(a)))\n",
    "    pt2 = (int(x0 - 1000*(-b)),int(y0 - 1000*(a)))\n",
    "    # Draws a line segment connecting two points, colour=(255,0,0) and thickness=2.\n",
    "    cv2.line(color_board,pt1,pt2,(255,0,0),2)\n",
    "\n",
    "                                                  \n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(checkerboard, cmap='gray')  \n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(board_edge, cmap='gray')  \n",
    "plt.title('Canny edge map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(color_board) \n",
    "plt.title(\"Detected Lines (in red)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG5kTM3DK5d_"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "- Use SIFT+RANSAC to match the close-up views of buildings to the larger scenes. Note that this may require changing the ratio used in the Lowe's ratio test step of FLANN matcher and/or the maxIters parameter of the RANSAC step. \n",
    "- What is the effect of changing these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "aaFdgXzPK5eA",
    "outputId": "a64e598c-3e10-4e03-ada8-9d1e84036acc"
   },
   "outputs": [],
   "source": [
    "# Read in close-up views of buildings images and the larger scenes images as graycsale.\n",
    "rootpath='./'\n",
    "flinders1 = cv2.imread(os.path.join(rootpath, \"flinders1.png\"),cv2.IMREAD_GRAYSCALE) \n",
    "flinders2 = cv2.imread(os.path.join(rootpath, \"flinders2.png\"),cv2.IMREAD_GRAYSCALE) \n",
    "unimelb1 = cv2.imread(os.path.join(rootpath, \"unimelb1.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "unimelb2 = cv2.imread(os.path.join(rootpath, \"unimelb2.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Display original image and scene image\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(flinders1, cmap='gray')  \n",
    "plt.title('Close-up views of Flinders Station ')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(flinders2, cmap='gray')  \n",
    "plt.title('Larger scenes of Flinders Station')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(unimelb1, cmap='gray')  \n",
    "plt.title('Close-up views of Unimelb Old Arts Clock Tower')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(unimelb2, cmap='gray')  \n",
    "plt.title('Larger scenes of Unimelb Old Arts Clock Tower')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMdYrLgnK5eA"
   },
   "source": [
    "### Exercise 2 Solution\n",
    "\n",
    "Answer: Both of these are a bit more difficult than the box scene, so they may require a different ratio (which changes the number of good matches and percentage of false matches). They may also require a higher number of RANSAC iterations to find a correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG8eNq5RK5eA"
   },
   "source": [
    "The code here is only a reference solution. The aim of the exercise is to understand the effect of changing these parameters. Different parameters are accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4fs1ktYK5eA"
   },
   "source": [
    "Flinders Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "bg4YHxTUK5eA",
    "outputId": "774efc94-16ca-42c3-f864-c32f2b267ad1"
   },
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create() # if cv2 version >= 4.4.0 \n",
    "# sift = cv2.xfeatures2d.SIFT_create() # if cv2 version = 4.3.x \n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(flinders1,None)\n",
    "kp2, des2 = sift.detectAndCompute(flinders2,None)\n",
    "\n",
    "# FLANN parameters and initialize\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Matching descriptor using KNN algorithm\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all good matches as per Lowe's Ratio test.\n",
    "ratio = 0.6\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < ratio*n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# Set a condition that at least MIN_MATCH_NUM matches are required to find the object. \n",
    "MIN_MATCH_NUM = 4\n",
    "\n",
    "if len(good)> MIN_MATCH_NUM:\n",
    "    # If enough matches are found, we extract the positions of the matched keypoints in both images. \n",
    "    # They are passed to find the perspective transformation. \n",
    "    \n",
    "    # Estimate homography between two images\n",
    "    ptsA = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    ptsB = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    H, status = cv2.findHomography(ptsA, \n",
    "                                   ptsB, \n",
    "                                   cv2.RANSAC, \n",
    "                                   ransacReprojThreshold = 5, \n",
    "                                   maxIters = 500)\n",
    "\n",
    "    matchesMask = status.ravel().tolist()\n",
    "\n",
    "    # Draw detected template in scene image\n",
    "    imgOut = cv2.warpPerspective(flinders2, H, (flinders1.shape[1],flinders1.shape[0]),\n",
    "                             flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "    \n",
    "    # Print total number of successful matches between two images\n",
    "    print(\"\\nNumber of successful matches between two images: \", matchesMask.count(1)) # Returns the number of 1 in the success list\n",
    "\n",
    "else:\n",
    "    # Otherwise, print that “Not enough matches are found”.\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_NUM) )\n",
    "    matchesMask = None\n",
    "\n",
    "\n",
    "# Draw our inliers (if successfully found the object) or all matching keypoints (if failed)\n",
    "draw_params = dict(matchColor = (0,255,0), # draw in a specific colour\n",
    "                   singlePointColor = (255,0,0), # draw in a specific colour\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "success_matches = cv2.drawMatches(flinders1,kp1,flinders2,kp2,good,None,**draw_params)\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "if matchesMask == None:\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All matching keypoints')\n",
    "    plt.axis('off')\n",
    "    \n",
    "else:\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All successful matches')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(imgOut, 'gray')\n",
    "    plt.title('Display detected template in scene image')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxHfUGLGK5eA"
   },
   "source": [
    "Unimelb Old Arts Clock Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "dMT0KHTIK5eA",
    "outputId": "9769902c-4b9a-4948-968f-3bc938e6050d"
   },
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create() # if cv2 version >= 4.4.0 \n",
    "# sift = cv2.xfeatures2d.SIFT_create() # if cv2 version = 4.3.x \n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(unimelb1,None)\n",
    "kp2, des2 = sift.detectAndCompute(unimelb2,None)\n",
    "\n",
    "# FLANN parameters and initialize\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Matching descriptor using KNN algorithm\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all good matches as per Lowe's Ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# Set a condition that at least MIN_MATCH_NUM matches are required to find the object. \n",
    "MIN_MATCH_NUM = 4\n",
    "\n",
    "if len(good)> MIN_MATCH_NUM:\n",
    "    # If enough matches are found, we extract the positions of the matched keypoints in both images. \n",
    "    # They are passed to find the perspective transformation. \n",
    "    \n",
    "    # Estimate homography between two images\n",
    "    ptsA = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    ptsB = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    H, status = cv2.findHomography(ptsA, \n",
    "                                   ptsB, \n",
    "                                   cv2.RANSAC, \n",
    "                                   ransacReprojThreshold = 5, \n",
    "                                   maxIters = 200)\n",
    "\n",
    "    matchesMask = status.ravel().tolist()\n",
    "\n",
    "    # Draw detected template in scene image\n",
    "    imgOut = cv2.warpPerspective(unimelb2, H, (unimelb1.shape[1],unimelb1.shape[0]),\n",
    "                             flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "    \n",
    "    # Print total number of successful matches between two images\n",
    "    print(\"\\nNumber of successful matches between two images: \", matchesMask.count(1)) # Returns the number of 1 in the success list\n",
    "\n",
    "else:\n",
    "    # Otherwise, print that “Not enough matches are found”.\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_NUM) )\n",
    "    matchesMask = None\n",
    "\n",
    "\n",
    "# Draw our inliers (if successfully found the object) or all matching keypoints (if failed)\n",
    "draw_params = dict(#matchColor = (0,255,0), # draw in a specific colour\n",
    "                   #singlePointColor = (255,0,0), # draw in a specific colour\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "success_matches = cv2.drawMatches(unimelb1,kp1,unimelb2,kp2,good,None,**draw_params)\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "if matchesMask == None:\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All matching keypoints')\n",
    "    plt.axis('off')\n",
    "    \n",
    "else:\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All successful matches')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(imgOut, 'gray')\n",
    "    plt.title('Display detected template in scene image')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnYDa8TbK5eA"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "\n",
    "- Implement the RANSAC algorithm step by step by filling in the three main steps of the RANSAC loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ593DkXK5eA"
   },
   "source": [
    "### Exercise 3 Solution\n",
    "\n",
    "Adapted from: https://salzis.wordpress.com/2014/06/10/robust-linear-model-estimation-using-ransac-python-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T_eIL_iK5eA"
   },
   "source": [
    "Set up the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbPshkJHK5eA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    " \n",
    "# Ransac parameters\n",
    "ransac_iterations = 20  # number of iterations\n",
    "ransac_threshold = 3    # threshold\n",
    "ransac_ratio = 0.6      # ratio of inliers required to assert\n",
    "                        # that a model fits well to data\n",
    "\n",
    "# generate sparse input data\n",
    "n_samples = 500               # number of input points\n",
    "outliers_ratio = 0.4          # ratio of outliers\n",
    " \n",
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    " \n",
    "# generate samples\n",
    "x = 30*np.random.random((n_samples,n_inputs) )\n",
    " \n",
    "# generate line's slope (called here perfect fit)\n",
    "perfect_fit = 0.5*np.random.normal(size=(n_inputs,n_outputs) )\n",
    " \n",
    "# compute output\n",
    "y = np.dot(x,perfect_fit)\n",
    "\n",
    "# add a little gaussian noise\n",
    "x_noise = x + np.random.normal(size=x.shape)\n",
    "y_noise = y + np.random.normal(size=y.shape)\n",
    " \n",
    "# add some outliers to the point-set\n",
    "n_outliers = round(outliers_ratio*n_samples)\n",
    "indices = np.arange(x_noise.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "outlier_indices = indices[:n_outliers]\n",
    " \n",
    "x_noise[outlier_indices] = 30*np.random.random(size=(n_outliers,n_inputs))\n",
    " \n",
    "# gaussian outliers\n",
    "y_noise[outlier_indices] = 30*np.random.normal(size=(n_outliers,n_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47JuunRAK5eB"
   },
   "source": [
    "These two functions will be called by the RANSAC loop: `find_line_model()` fits a line to sample points and `dist_to_line()` computes the distance from a sample point to the closest point on a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyI1DW1fK5eB"
   },
   "outputs": [],
   "source": [
    "def find_line_model(points):\n",
    "    \"\"\" find a line model for the given points\n",
    "    :param points selected points for model fitting\n",
    "    :return line model\n",
    "    \"\"\"\n",
    " \n",
    "    # [WARNING] vertical and horizontal lines should be treated differently\n",
    "    #           here we just add some noise to avoid division by zero\n",
    " \n",
    "    # find a line model for these points\n",
    "    m = (points[1,1] - points[0,1]) / (points[1,0] - points[0,0] + sys.float_info.epsilon)  # slope (gradient) of the line\n",
    "    c = points[1,1] - m * points[1,0]                                     # y-intercept of the line\n",
    " \n",
    "    return m, c\n",
    "\n",
    "def find_dist_to_line(m, c, x0, y0):\n",
    "    \"\"\" find an intercept point of the line model with\n",
    "        a normal from point (x0,y0) to it, return\n",
    "        distance betwee point (x0, y0) and intercept\n",
    "    :param m slope of the line model\n",
    "    :param c y-intercept of the line model\n",
    "    :param x0 point's x coordinate\n",
    "    :param y0 point's y coordinate\n",
    "    :return intercept point\n",
    "    \"\"\"\n",
    " \n",
    "    # intersection point with the model\n",
    "    x = (x0 + m*y0 - m*c)/(1 + m**2)\n",
    "    y = (m*x0 + (m**2)*y0 - (m**2)*c)/(1 + m**2) + c\n",
    "    dist = math.sqrt((x - x0)**2 + (y - y0)**2)\n",
    " \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkVwmqSVK5eB"
   },
   "source": [
    "A function for plotting the RANSAC iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIwiygU0K5eB"
   },
   "outputs": [],
   "source": [
    "def ransac_plot(n, x, y, m, c, x_in=(), y_in=(), points=()):\n",
    "    \"\"\" plot the current RANSAC step\n",
    "    :param n      iteration\n",
    "    :param x      samples x\n",
    "    :param y      samples y\n",
    "    :param m      slope of the line model\n",
    "    :param c      shift of the line model\n",
    "    :param x_in   inliers x\n",
    "    :param y_in   inliers y\n",
    "    :param points picked up points for modeling\n",
    "    \"\"\"\n",
    "\n",
    "    line_width = 1.\n",
    "    line_color = '#0080ff'\n",
    "    title = 'iteration ' + str(n)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    # plot input points\n",
    "    plt.plot(x[:,0], y[:,0], marker='o', label='Input points', color='#00cc00', linestyle='None', alpha=0.4)\n",
    " \n",
    "    # draw the current model\n",
    "    plt.plot(x, m*x + c, 'r', label='Line model', color=line_color, linewidth=line_width)\n",
    " \n",
    "    # draw inliers, if provided\n",
    "    if len(x_in) > 0:\n",
    "        plt.plot(x_in, y_in, marker='o', label='Inliers', linestyle='None', color='#ff0000', alpha=0.6)\n",
    " \n",
    "    # draw points picked up for the modeling, if provided\n",
    "    if len(points) > 0:\n",
    "        plt.plot(points[:,0], points[:,1], marker='o', label='Picked points', color='#0000cc', linestyle='None', alpha=0.6)\n",
    " \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzB00SUwK5eB"
   },
   "source": [
    "The main RANSAC loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VSs9dqyKK5eB",
    "outputId": "78b0f82a-dd3b-4bf4-e28f-edf20b810e7c"
   },
   "outputs": [],
   "source": [
    "data = np.hstack( (x_noise,y_noise) )\n",
    " \n",
    "ratio = 0.\n",
    "model_m = 0.\n",
    "model_c = 0.\n",
    " \n",
    "# perform RANSAC iterations\n",
    "for it in range(ransac_iterations):\n",
    " \n",
    "    # randomly sample N points\n",
    "    n = 2\n",
    " \n",
    "    all_indices = np.arange(x_noise.shape[0])\n",
    "    np.random.shuffle(all_indices)\n",
    " \n",
    "    indices_1 = all_indices[:n]\n",
    "    indices_2 = all_indices[n:]\n",
    " \n",
    "    sample_points = data[indices_1,:]\n",
    "    test_points = data[indices_2,:]\n",
    " \n",
    "    # fit a line model to the sampled points\n",
    "    m, c = find_line_model(sample_points)\n",
    " \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    num = 0\n",
    "\n",
    "    # count the number of inliers num\n",
    "    # inliers are points whose distance to the line is less than ransac_threshold\n",
    "    for ind in range(test_points.shape[0]):\n",
    " \n",
    "        x0 = test_points[ind,0]\n",
    "        y0 = test_points[ind,1]\n",
    " \n",
    "        # find an intercept point of the model with a normal from point (x0,y0)\n",
    "        dist = find_dist_to_line(m, c, x0, y0)\n",
    " \n",
    "        # check whether it's an inlier or not\n",
    "        if dist < ransac_threshold:\n",
    "            x_list.append(x0)\n",
    "            y_list.append(y0)\n",
    "            num += 1\n",
    " \n",
    "    x_inliers = np.array(x_list)\n",
    "    y_inliers = np.array(y_list)\n",
    " \n",
    "    # if this value of num is higher than previously saved value,\n",
    "    # save it, and save the current model parameters\n",
    "    if num/float(n_samples) > ratio:\n",
    "        ratio = num/float(n_samples)\n",
    "        model_m = m\n",
    "        model_c = c\n",
    " \n",
    "    print('   inlier ratio = ', num/float(n_samples))\n",
    "    print('  model_m = ', model_m)\n",
    "    print('  model_c = ', model_c)\n",
    " \n",
    "    # plot the current step with inliers and sample points\n",
    "    ransac_plot(it, x_noise, y_noise, m, c, x_inliers, y_inliers, sample_points)\n",
    "    # plot the current step without showing inliers or sample points\n",
    "    #ransac_plot(0, x_noise, y_noise, model_m, model_c)\n",
    " \n",
    "    # we are done in case we have enough inliers\n",
    "    if num > n_samples*ransac_ratio:\n",
    "        print('The model is found !')\n",
    "        break\n",
    "\n",
    "# plot the final model\n",
    "ransac_plot(0, x_noise, y_noise, model_m, model_c)\n",
    " \n",
    "print('\\nFinal model:\\n')\n",
    "print('  ratio = ', ratio)\n",
    "print('  model_m = ', model_m)\n",
    "print('  model_c = ', model_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpcIEpwbK5eB"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "<img style=\"float:\" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/ball.png\" width=300>\n",
    "<img style=\"float:\" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/ball2.png\" width=300>\n",
    "\n",
    "An application needs to find a circle of unknown size in an image. It operates as follows:\n",
    "\n",
    "1. Detect edges\n",
    "2. RANSAC:\\\n",
    "    (1)\tSample 3 edge points  \n",
    "    (2) Fit a circle to those three points (three is enough to uniquely fit a circle)\\\n",
    "    (3)\tCount consensus\n",
    "    \n",
    "3. Report circle with maximum consensus\n",
    "    \n",
    "i) If the circle in the image has 10% of the edge pixels, calculate how many RANSAC iterations are needed to find it with 99% probability.\n",
    "\n",
    "A: log(0.01)/log(0.999) = 4603\n",
    "\n",
    "ii) How might the algorithm be modified to improve the accuracy of the estimated centre and radius of the circle?\n",
    "\n",
    "A: Re-estimate using all inliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "worksheet07_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
